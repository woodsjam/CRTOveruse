Looking at Taylor Data
========================================================
Load in the data
```{r}
Original <- read.csv("~/Dropbox/Research/CRTOveruse/OriginalData/Original.csv")

#Original <- read.csv("~/Research/CRTOveruse/OriginalData/Original.csv")

```

Manipulations based on description
```{r}
#Fix the ids
Original$id<-as.factor(Original$id)

#make a logical out of the treatment variable
Original$HypoTRUE<-ifelse(Original$treatment=='1',TRUE,FALSE)

#Recode the dummy variable sequence for major
Original$Major<-NA
Original$Major[Original$dum_major1=='1']<-"Econ"
Original$Major[Original$dum_major2=='1']<-"Env"
Original$Major[Original$dum_major3=='1']<-"Sci"
Original$Major[Original$dum_major4=='1']<-"Bus"
Original$Major[Original$dum_major5=='1']<-"Other"
Original$Major<-as.factor(Original$Major)

# Testing out how to use other distributions for estimator.



#Recode responses as logicals
Original$safereal1<-as.logical(Original$safereal1)
Original$safereal2<-as.logical(Original$safereal2)
Original$safereal3<-as.logical(Original$safereal3)
Original$safereal4<-as.logical(Original$safereal4)
Original$safereal5<-as.logical(Original$safereal5)
Original$safereal6<-as.logical(Original$safereal6)
Original$safereal7<-as.logical(Original$safereal7)
Original$safereal8<-as.logical(Original$safereal8)
Original$safereal9<-as.logical(Original$safereal9)
Original$safereal10<-as.logical(Original$safereal10)

Original$safehypot1<-as.logical(Original$safehypot1)
Original$safehypot2<-as.logical(Original$safehypot2)
Original$safehypot3<-as.logical(Original$safehypot3)
Original$safehypot4<-as.logical(Original$safehypot4)
Original$safehypot5<-as.logical(Original$safehypot5)
Original$safehypot6<-as.logical(Original$safehypot6)
Original$safehypot7<-as.logical(Original$safehypot7)
Original$safehypot8<-as.logical(Original$safehypot8)
Original$safehypot9<-as.logical(Original$safehypot9)
Original$safehypot10<-as.logical(Original$safehypot10)

summary(Original)
```
There could be some tricks because Taylor added some noise to the choices and I don't know the exact noise.


# Prototype an estimator


Functions we will need.
```{r}

# the utility function
CRRA<-function(w,r){w^(1-r)/(1-r)}

#expected utility of lottery
EU<-function(w1,u1,w2,u2){(w1*u1) +(w2*u2)}

#The two latent variable specifications.  
Fechner<-function(UtilityL, UtilityR, mu){
  (UtilityR-UtilityL)/mu
}

Luce<-function(UtilityL, UtilityR, mu){
  UtilityR^(1/mu)/(UtilityR^(1/mu) +UtilityL^(1/mu))
}

```

Note that taylor only got Fechner to really work on the data because of convergence issues.


We need the values of the lotteries.  Here are the choices from table 1 of Taylor's paper.  The safe gambles are for $40 and $32 and the risky is $77 and $2.  Note that they added some noise to the values so these are just expectations.

```{r}

Choices<-data.frame(
  rbind(c(1, 0.1, 0.9, 0.1, 0.9, 32.80, 9.50),
        c(2, 0.2, 0.8, 0.2, 0.8, 33.60, 17.00),
        c(3, 0.3, 0.7, 0.3, 0.7, 34.40, 24.80),
        c(4, 0.4, 0.6, 0.4, 0.6, 35.20, 32.00),
        c(5, 0.5, 0.5, 0.5, 0.5, 36.00, 39.50),
        c(6, 0.6, 0.4, 0.6, 0.4, 36.80, 47.00),
        c(7, 0.7, 0.3, 0.7, 0.3, 37.60, 54.50),
        c(8, 0.8, 0.2, 0.8, 0.2, 38.40, 62.00),
        c(9, 0.9, 0.1, 0.9, 0.1, 39.20, 69.50),
        c(10, 1, 0, 1, 0, 40.00, 77.00)
        )
  )

names(Choices)<-c("Choice","PSafeHigh","PSafeLow","PRiskyHigh","PRiskyLow","ExpectSafe","ExpectRisky")


```


Here is the variable list for both the r and mu equations from the paper.


* 1(Hypot) 
* 1(Female) 
* 1(Female)×1(Hypot)
* CRT Score 
* CRT×1(Hypot) 
* Numeracy Score 
* Numeracy×1(Hypot) 
* 1(EV-sufficient All Choices) 
* 1(EV-sufficient All)×1(Hypot) 


Note that EV-sufficient variable and Numeracy are missing so I will ommit.  He also does not use major, which he has given, me but does use gender, which he has not.  I can't replicate so I will use what I can.

* 1(Hypot) 
* CRT Score 
* CRT×1(Hypot) 

The limited list makes it easy and I can avoid stepping on toes.

Lets get the dataset into shape.
```{r}
#Just the variables that I need

MinimalSubset<-Original[,c("id","crt","treatment","safereal1","safereal2","safereal3","safereal4","safereal5","safereal6","safereal7","safereal8","safereal9","safereal10","safehypot1","safehypot2","safehypot3","safehypot4","safehypot5","safehypot6","safehypot7","safehypot8","safehypot9","safehypot10")]

library(reshape2)

#Turn this into long form.
LongForm<-melt(MinimalSubset,id.vars=c('id','treatment','crt'))

LongForm$Choice<-LongForm$variable

library(stringr)
LongForm$Choice<-str_replace(LongForm$Choice,'safereal','')
LongForm$Choice<-str_replace(LongForm$Choice,'safehypot','')
LongForm$Choice<-as.factor(LongForm$Choice)
#Add values from the Choices table

LongForm<-merge(LongForm, Choices[,c('Choice','ExpectSafe','ExpectRisky')], by='Choice')

summary(LongForm)
```

That should be in a usable form now.

# Test of simulator estimator for a linear model

I asked on stackexchange if anyone had a reference for errors in variables when the distribution of the error was known.  Until an answer showes up, I will work with a simulation estimator.


Fake data for test.

```{r}
Fake<-data.frame(rnorm(100),rnorm(100))
names(Fake)<-c('X1','XTrue')

Fake$Noise<-rgamma(100,shape=2, rate=2)
Fake$X2<-Fake$XTrue+Fake$Noise

Fake$epsilon<-rnorm(100)

Fake$Y<- 10 + 5*Fake$X1 + 3*Fake$XTrue +Fake$epsilon

summary(Fake)

```

Lets see what OLS turns up

```{r}
summary(lm(Y~X1+X2,data=Fake))

```

Bias all over the place.

Now lets get an estimator in place for this linear model

```{r}

Augment<-function(variable, shape, rate){
  variable-rgamma(length(variable), shape, rate)
}

summary(lm(Y~X1+Augment(X2,2,2),data=Fake))

```

So, even with the shape and rate paramters known with certainty, I don't find the true parameter on the X2 variable.  

Lets run it a few times to make sure.  It may show up as an expectation.

```{r}

summary(replicate(400,lm(Y~X1+Augment(X2,2,2),data=Fake)$coef[3]))

```
and the answer is no.

## Simulation idea but with discrete overstatement

This plays off a new idea that the overstatement is discrete.  If we model the overstatement as multiplicative then we can't identify, but if we work it as additive we may be able to.

Time for a new Fake dataset

```{r}
rm(Fake)
Fake<-data.frame(rnorm(100),rhyper(100, 200,1000,3))
names(Fake)<-c('X1','XTrue')

Fake$Noise<-rhyper(100, 200,1000,3)
Fake$X2<-pmin(3,Fake$XTrue+Fake$Noise)

Fake$epsilon<-rnorm(100)

Fake$Y<- 10 + 5*Fake$X1 + 3*Fake$XTrue +Fake$epsilon

summary(Fake)

```

Test the sigmoid wrap
```{r}
summary((Fake$X2-1)/(1-exp(-5*(Fake$X2-1))))
summary((Fake$X2-2)/(1-exp(-5*(Fake$X2-2))))
summary((Fake$X2-3)/(1-exp(-5*(Fake$X2-3))))

```
Why NaN?


```{r}
data.frame(Fake$X2, (Fake$X2-1)/(1-exp(-5*(Fake$X2-1))))

(0)/(1-exp(-0))
```

Because r does not know L'Hospital's Rule.  It breaks when the N is an integer value.  We just need non-integer starting values

Now lets try an estimator

```{r}

# summary(nls(Y~alpha+beta1*X1+beta2*(X2-N)/(1-exp(-10*(X2-N))),data=Fake, start=list(alpha=10,beta1=5, beta2=3, N=1.1)))

summary(lm(Y~X1+X2,data=Fake))

```
This has a fragile convergence radius.  You need to be pretty close to get it to work.  The other thing is that the assumption of an average amount for each, homogenous reduction, is messing things up.

Need to work on the probability of reduction like a tobit model.  A puzzle for another day.

## Different levels of overstatement by level


Back with another idea...lets have the cheating paramters be different depending on the observed score.

```{r}
#Still sigular gradient at start
summary(nls(Y~alpha+beta1*X1+beta23*I(X2==3)*(X2-N3)/(1-exp(-10*(X2-N3))) +beta22*I(X2==2)*(X2-N2)/(1-exp(-10*(X2-N2)))+beta21*I(X2==1)*(X2-N1)/(1-exp(-10*(X2-N1))),data=Fake, start=list(alpha=10, beta1=5, beta21=1, N1=.1, beta22=2, N2=.2, beta23=3, N3=.3),trace=TRUE))


```
Good idea but it will not converge. 

Lets deal with the point discontinuity.

## Point discontinuity


This should fix the point discontinuity.
```{r}
wrap<-function(val,N){
  ret<-.1
  if(N!=val) {
    ret<- (val-N)/(1- exp (-10 * (val-N)))
  }
  ret
}

Wrap<-Vectorize(wrap)
```

Try the regressions again
```{r}

```

summary(nls(Y~alpha+beta1*X1+beta2*Wrap(X2,N),data=Fake, start=list(alpha=10,beta1=5, beta2=3, N=1.1),trace=TRUE))

summary(lm(Y~X1+X2,data=Fake))
